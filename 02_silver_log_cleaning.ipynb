{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "940dad68-76ba-4a50-a2a0-0f17f37f3bdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark.sql(\"USE CATALOG real_time_projects\")\n",
    "spark.sql(\"USE SCHEMA realtime_log_analytics_alerting\")\n",
    "\n",
    "silver_df = (\n",
    "    spark.readStream.table(\"bronze_logs\")\n",
    "    .withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
    "    .filter(col(\"timestamp\").isNotNull())\n",
    "    .withColumn(\"level\", upper(col(\"level\")))\n",
    "    .withColumn(\"log_date\", to_date(\"timestamp\"))\n",
    ")\n",
    "\n",
    "silver_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", \"/Volumes/real_time_projects/realtime_log_analytics_alerting/project_volume/checkpoints/silver\") \\\n",
    "    .partitionBy(\"log_date\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .table(\"silver_logs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04793fd9-f57f-4ef5-aefa-7156f735db5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#------------------ Data Quality Checks ------------\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "valid_levels = [\"INFO\", \"WARN\", \"ERROR\"]\n",
    "\n",
    "spark.sql(\"USE CATALOG real_time_projects\")\n",
    "spark.sql(\"USE SCHEMA realtime_log_analytics_alerting\")\n",
    "\n",
    "dq_df = (\n",
    "    spark.readStream.table(\"silver_logs\")\n",
    "    .withColumn(\n",
    "        \"dq_error\",\n",
    "        when(col(\"event_id\").isNull(), \"NULL_EVENT_ID\")\n",
    "        .when(col(\"service\").isNull(), \"NULL_SERVICE\")\n",
    "        .when(~col(\"level\").isin(valid_levels), \"INVALID_LEVEL\")\n",
    "        .when(col(\"response_time_ms\") <= 0, \"INVALID_RESPONSE_TIME\")\n",
    "        .when(col(\"timestamp\") > current_timestamp(), \"FUTURE_TIMESTAMP\")\n",
    "        .otherwise(\"VALID\")\n",
    "    )\n",
    ")\n",
    "\n",
    "#------------- Valid Stream -----------------\n",
    "dq_df.filter(col(\"dq_error\") == \"VALID\") \\\n",
    "    .drop(\"dq_error\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", \"/Volumes/real_time_projects/realtime_log_analytics_alerting/project_volume/checkpoints/silver_valid\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .table(\"silver_logs_valid\")\n",
    "\n",
    "#------------ Quarantine Stream ----------------\n",
    "dq_df.filter(col(\"dq_error\") != \"VALID\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", \"/Volumes/real_time_projects/realtime_log_analytics_alerting/project_volume/checkpoints/quarantine\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .table(\"quarantine_logs\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "528c8cd9-2d43-4e03-980c-0565a789742d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "#------------- SCD Type-2 Initial Load ----------------\n",
    "\n",
    "service_data = [\n",
    "    (\"payment-service\", \"payments\", \"critical\"),\n",
    "    (\"order-service\", \"orders\", \"high\"),\n",
    "    (\"inventory-service\", \"inventory\", \"medium\"),\n",
    "    (\"auth-service\", \"security\", \"critical\")\n",
    "]\n",
    "\n",
    "columns = [\"service\", \"owner\", \"tier\"]\n",
    "\n",
    "spark.sql(\"USE CATALOG real_time_projects\")\n",
    "spark.sql(\"USE SCHEMA realtime_log_analytics_alerting\")\n",
    "\n",
    "df = spark.createDataFrame(service_data, columns) \\\n",
    "    .withColumn(\"effective_from\", current_timestamp()) \\\n",
    "    .withColumn(\"effective_to\", lit(None).cast(\"timestamp\")) \\\n",
    "    .withColumn(\"is_current\", lit(True))\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"dim_service\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4899d01-b75a-4817-98ff-48cec069c522",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "#---------------- SCD Type-2 Incremental Load ------------------\n",
    "\n",
    "updates = [\n",
    "    (\"order-service\", \"orders\", \"critical\")  # tier changed\n",
    "]\n",
    "\n",
    "updates_df = spark.createDataFrame(updates, [\"service\", \"owner\", \"tier\"])\n",
    "\n",
    "delta_table = DeltaTable.forName(spark, \"dim_service\")\n",
    "\n",
    "delta_table.alias(\"t\").merge(\n",
    "    updates_df.alias(\"s\"),\n",
    "    \"t.service = s.service AND t.is_current = true\"\n",
    ").whenMatchedUpdate(\n",
    "    condition=\"t.tier <> s.tier\",\n",
    "    set={\n",
    "        \"effective_to\": current_timestamp(),\n",
    "        \"is_current\": lit(False)\n",
    "    }\n",
    ").whenNotMatchedInsert(\n",
    "    values={\n",
    "        \"service\": \"s.service\",\n",
    "        \"owner\": \"s.owner\",\n",
    "        \"tier\": \"s.tier\",\n",
    "        \"effective_from\": current_timestamp(),\n",
    "        \"effective_to\": lit(None),\n",
    "        \"is_current\": lit(True)\n",
    "    }\n",
    ").execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31b9b5b8-3e4c-4133-bf77-f3e0ccfbdfaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#---------- Enrichment (Streaming Join) -------------\n",
    "\n",
    "spark.sql(\"USE CATALOG real_time_projects\")\n",
    "spark.sql(\"USE SCHEMA realtime_log_analytics_alerting\")\n",
    "\n",
    "silver_logs = (\n",
    "    spark.readStream.table(\"silver_logs_valid\")\n",
    "    .withColumnRenamed(\"service\", \"silver_service\")\n",
    ")\n",
    "\n",
    "enriched_df = (\n",
    "    silver_logs.join(\n",
    "        spark.table(\"dim_service\"),\n",
    "        (col(\"silver_service\") == col(\"dim_service.service\")) &\n",
    "        (col(\"dim_service.is_current\") == True),\n",
    "        \"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "enriched_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", \"/Volumes/real_time_projects/realtime_log_analytics_alerting/project_volume/checkpoints/enriched\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .table(\"enriched_logs\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_silver_log_cleaning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
