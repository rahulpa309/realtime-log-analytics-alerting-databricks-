# üöÄ Real-Time Log Analytics & Alerting System

Production-style **real-time log monitoring pipeline** built using **PySpark Structured Streaming, Delta Lake, and Databricks Community Edition**. The project simulates Kafka-style ingestion, applies **data quality checks**, uses **SCD Type-2 dimensions**, and generates **real-time alerts and analytics**.

---

## üìå Business Use Case

Modern applications generate massive volumes of logs. Operations teams need to:

* Detect error spikes in real time
* Monitor application performance
* Trigger alerts automatically
* Analyze historical trends for RCA (Root Cause Analysis)

This project simulates a **cloud-scale log analytics platform** used by SaaS companies, IT services, banks, and e-commerce platforms.

---

## üèóÔ∏è Architecture Overview

```
Log Generator (Kafka-like)
        ‚Üì
Databricks Auto Loader
        ‚Üì
Bronze Delta Table (Raw Logs)
        ‚Üì
Silver Delta Table (Clean + Validated Logs)
        ‚Üì
SCD-2 Service Dimension
        ‚Üì
Gold Delta Table (Aggregated Metrics)
        ‚Üì
Alerts + Dashboards
```

---

## üß∞ Tech Stack

| Layer         | Technology                   |
| ------------- | ---------------------------- |
| Streaming     | PySpark Structured Streaming |
| Storage       | Delta Lake                   |
| Platform      | Databricks Community Edition |
| Data Quality  | Custom PySpark DQ Rules      |
| Modeling      | SCD Type-2                   |
| Visualization | Databricks SQL               |

---

## üìÅ Project Structure

```
Real-Time-Log-Analytics/
‚îÇ
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 00_log_generator
‚îÇ   ‚îú‚îÄ‚îÄ 01_bronze_log_ingestion
‚îÇ   ‚îú‚îÄ‚îÄ 02_silver_log_cleaning
‚îÇ         ‚îú‚îÄ‚îÄ 02.1_data_quality_checks
‚îÇ         ‚îú‚îÄ‚îÄ 02.2_scd2_service_metadata
‚îÇ   ‚îú‚îÄ‚îÄ 03_gold_aggregations
‚îÇ   ‚îú‚îÄ‚îÄ 04_alerting_logic
‚îÇ   ‚îî‚îÄ‚îÄ 05_dashboard_queries
‚îÇ
‚îî‚îÄ‚îÄ README.md
```

---

## üßæ Log Schema

```json
{
  "event_id": "uuid",
  "timestamp": "ISO-8601",
  "service": "payment-service",
  "level": "INFO | WARN | ERROR",
  "message": "string",
  "host": "server-01",
  "response_time_ms": 1200
}
```

---

## ü•â Bronze Layer ‚Äì Raw Ingestion

* Ingest logs continuously using Auto Loader
* Schema enforced, no transformation
* Fault-tolerant with checkpoints

**Output:** `bronze_logs`

---

## ü•à Silver Layer ‚Äì Cleaning & Standardization

* Convert timestamps
* Normalize log levels
* Add partition column (`log_date`)
* Drop malformed records

**Output:** `silver_logs`

---

## ‚úÖ Data Quality Checks (Critical Feature)

### Why Data Quality?

In real systems, **bad data = bad alerts**. This layer ensures only valid data moves forward.

---

### üìè Data Quality Rules Implemented

| Rule                  | Description                        |
| --------------------- | ---------------------------------- |
| Not Null              | `event_id`, `timestamp`, `service` |
| Valid Enum            | `level ‚àà (INFO, WARN, ERROR)`      |
| Range Check           | `response_time_ms > 0`             |
| Freshness             | Timestamp not in future            |
| Referential Integrity | Service exists in dimension        |

---

### üß™ DQ Implementation (PySpark)

```python
from pyspark.sql.functions import *

valid_levels = ['INFO', 'WARN', 'ERROR']

validated_df = (
    spark.readStream.table("silver_logs")
    .withColumn("dq_error",
        when(col("event_id").isNull(), "NULL_EVENT_ID")
        .when(col("service").isNull(), "NULL_SERVICE")
        .when(~col("level").isin(valid_levels), "INVALID_LEVEL")
        .when(col("response_time_ms") <= 0, "INVALID_RESPONSE_TIME")
        .when(col("timestamp") > current_timestamp(), "FUTURE_TIMESTAMP")
        .otherwise("VALID")
    )
)
```

---

### üü¢ Valid Records

```python
validated_df.filter(col("dq_error") == "VALID") \
  .writeStream \
  .format("delta") \
  .option("checkpointLocation", "/FileStore/checkpoints/silver_valid") \
  .table("silver_logs_valid")
```

---

### üî¥ Invalid Records (Quarantine)

```python
validated_df.filter(col("dq_error") != "VALID") \
  .writeStream \
  .format("delta") \
  .option("checkpointLocation", "/FileStore/checkpoints/quarantine") \
  .table("quarantine_logs")
```

**Benefit:** Bad data is isolated, not lost.

---

## üß¨ SCD Type-2 Service Dimension

Tracks historical changes in service ownership and criticality.

**Columns:**

* `service`
* `owner`
* `tier`
* `effective_from`
* `effective_to`
* `is_current`

Used to enrich logs with **current service metadata**.

---

## ü•á Gold Layer ‚Äì Aggregations

Metrics computed every **5-minute window**:

* Error count per service
* Average response time
* Log volume trends

**Output:** `gold_log_metrics`

---

## üö® Alerting Logic

### Alert Conditions

| Metric            | Threshold      |
| ----------------- | -------------- |
| ERROR count       | > 50 in 5 mins |
| Avg response time | > 2000 ms      |

Alerts are written to `alerts_table` and can be integrated with:

* Slack webhook
* Email

---

## üìä Dashboards

Built using Databricks SQL:

* Errors by service
* Error trend over time
* Response time heatmap
* Top failing services

---

## ‚öôÔ∏è Performance Optimizations

* Partitioning by `log_date`
* Watermarking for late data
* Delta Lake checkpointing
* Z-ORDER on `service`, `level`
* Broadcast join for service dimension

---

## üìà Business Impact

* Faster incident detection
* Reduced MTTR (Mean Time to Resolution)
* Improved SLA compliance
* Production-ready streaming design

---

## ‚≠ê Future Enhancements

* Great Expectations integration
* Schema drift detection
* ML-based anomaly detection
* Cloud deployment (AWS/Azure)

---
